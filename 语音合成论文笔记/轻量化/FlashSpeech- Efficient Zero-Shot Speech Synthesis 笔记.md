> preprint 2024.4 港科大、Microsoft
<!-- 翻译 & 理解 -->
<!-- Recent progress in large-scale zero-shot speech synthesis has been significantly advanced by language models and diffusion models. However, the generation process of both methods is slow and computationally intensive. Efficient speech synthesis using a lower computing budget to achieve quality on par with previous work remains a significant challenge. In this paper, we present FlashSpeech, a large-scale zero-shot speech synthesis system with approximately 5% of the in- ference time compared with previous work. FlashSpeech is built on the latent consistency model and applies a novel adversarial consistency training approach that can train from scratch without the need for a pre-trained diffusion model as the teacher. Furthermore, a new prosody generator module enhances the diversity of prosody, making the rhythm of the speech sound more natural. The generation processes of FlashSpeech can be achieved efficiently with one or two sampling steps while maintaining high audio quality and high similarity to the audio prompt for zero-shot speech generation. Our experimental results demonstrate the supe- rior performance of FlashSpeech. Notably, FlashSpeech can be about 20 times faster than other zero-shot speech synthesis systems while maintaining comparable performance in terms of voice quality and similarity. Furthermore, FlashSpeech demonstrates its versatility by efficiently performing tasks like voice conversion, speech editing, and diverse speech sampling. Audio samples can be found in https://flashspeech.github.io/. -->
1. 提出 FlashSpeech，大规模 zero-shot 语音合成，相比于之前的工作，只要 5% 的推理时间
2. FlashSpeech 基于 latent consistency model，采用新的对抗一致性训练方法，无需预训练的 diffusion 模型作为 teacher
3. 新的 prosody generator 模块可以提升韵律多样性
4. 可以在一两个采样步内高效生成语音，同时保持高音质和高相似性

## Introduction
<!-- In recent years, the landscape of speech synthesis has been transformed by the advent of large-scale generative models. Consequently, the latest research efforts have achieved notable advancements in zero-shot speech synthesis systems by significantly increasing the size of both datasets and models. Zero-shot speech synthesis, such as text-to-speech (TTS), voice conversion (VC) and Editing, aims to generate speech that incorporates unseen speaker characteristics from a reference audio segment during inference, without the need for additional training. Current advanced zero-shot speech synthesis systems typically leverage language models (LMs) Wang et al. (2023a); Yang et al. (2023); Zhang et al. (2023); Kharitonov et al. (2023); Wang et al. (2023b); Peng et al. (2024); Kim et al. (2024) and diffusion-style models Shen et al. (2024); Kim et al. (2023b); Le et al. (2023); Jiang et al. (2023b) for in-context speech generation on the large-scale dataset. However, the generation process of these methods needs a long-time iteration. For example, VALL-E Wang et al. (2023a) builds on the language model to predict 75 audio token sequences for a 1-second speech, in its first-stage autoregressive (AR) token sequence generation. When using a non-autoregressive (NAR) latent diffusion model Rombach et al. (2022) based framework, NaturalSpeech 2 Shen et al. (2024) still requires 150 sampling steps. As a result, although these methods can produce human-like speech, they require significant computational time and cost. Some efforts have been made to accelerate the generation process. Voicebox Le et al. (2023) adopts flow-matching Lipman et al. (2022) so that fewer sampling steps (NFE1: 64) can be achieved because of the optimal transport path. ClaM-TTS Kim et al. (2024) proposes a mel-codec with a superior compression rate and a latent language model that generates a stack of tokens at once. Although the slow generation speed issue has been somewhat alleviated, the inference speed is still far from satisfactory for practical applications. Moreover, the substantial computational time of these approaches leads to significant computational cost overheads, presenting another challenge. -->
<!-- The fundamental limitation of speech generation stems from the intrinsic mechanisms of language models and diffusion models, which require considerable time either auto-regressively or through a large number of denoising steps. Hence, the primary objective of this work is to accelerate inference speed and reduce computational costs while preserving generation quality at levels comparable to the prior research. In this paper, we propose FlashSpeech as the next step towards efficient zero- shot speech synthesis. To address the challenge of slow generation speed, we leverage the latent consistency model (LCM) Luo et al. (2023), a recent advancement in generative models. Building upon the previous non-autoregressive TTS system Shen et al. (2024), we adopt the encoder of a neural audio codec to convert speech waveforms into latent vectors as the training target for our LCM. To train this model, we propose a novel technique called adversarial consistency training, which utilizes the capabilities of pre-trained speech language models Chen et al. (2022b); Hsu et al. (2021); Baevski et al. (2020) as discriminators. This facilitates the transfer of knowledge from large pre-trained speech language models to speech generation tasks, efficiently integrating adversarial and consistency training to improve performance. The LCM is conditioned on prior vectors obtained from a phoneme encoder, a prompt encoder, and a prosody generator. Furthermore, we demonstrate that our proposed prosody generator leads to more diverse expressions and prosody while preserving stability. -->
1. 合成的限制来源于 LM 和 diffusion 模型，要么需要大量时间自回归，要么需要大量去噪步
2. 提出 FlashSpeech，采用 latent consistency model（LCM），基于 [NaturalSpeech 2- Latent Diffusion Models are Natural and Zero-Shot Speech and Singing Synthesizers 笔记](../NaturalSpeech%202-%20Latent%20Diffusion%20Models%20are%20Natural%20and%20Zero-Shot%20Speech%20and%20Singing%20Synthesizers%20笔记.md)，提出 adversarial consistency training，采用预训练的 speech LM 作为判别器，提高性能，贡献如下：
<!-- We propose FlashSpeech, an efficient zero-shot speech synthesis system that generates voice with high audio quality and speaker similarity in zero-shot scenarios. -->
    + 提出 FlashSpeech，高效 zero-shot 语音合成系统，生成高音质和说话者相似性的语音
<!-- We introduce adversarial consistency training, a novel combination of consistency and adversarial training leveraging pre-trained speech language models, for training the latent consistency model from scratch, achieving speech generation in one or two steps. -->
    + 引入 adversarial consistency training，结合一致性和对抗训练，利用预训练的 speech LM，从头训练 latent consistency model，实现一两步内生成语音
<!--  We propose a prosody generator module that enhances the diversity of prosody while maintaining stability. -->
    + 提出 prosody generator 模块，提高韵律多样性，保持稳定性
<!-- FlashSpeech significantly outperforms strong baselines in audio quality and matches them in speaker similarity. Remarkably, it achieves this at a speed approximately 20 times faster than comparable systems, demonstrating unprecedented efficiency. -->
    + FlashSpeech 在音频质量上优于 baseline，速度快 20 倍

## 相关工作（略）

## FlashSpeech
