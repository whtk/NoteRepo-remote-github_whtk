- [x] FreGrad- Lightweight and Fast Frequency-aware Diffusion Vocoder
- [x] Matcha-TTS- A fast TTS architecture with conditional flow matching
- [x] Period VITS- Variational Inference with Explicit Pitch Modeling for End-to-end Emotional Speech Synthesis
- [x] DiffProsody- Diffusion-based Latent Prosody Generation for Expressive Speech Synthesis with Prosody Conditional Adversarial Training
- [ ] Hierarchical Prosody Modeling for Non-Autoregressive Speech Synthesis
- [ ] NaturalSpeech 3: Zero-Shot Speech Synthesis with Factorized Codec and Diffusion Models
- [ ] Expressive TTS Training with Frame and Style Reconstruction Loss
- [ ] PauseSpeech: Natural Speech Synthesis via Pre-trained Language Model and Pause-based Prosody Modelin
- [ ] Meta-StyleSpeech : Multi-Speaker Adaptive Text-to-Speech Generation
- [ ] Disentangling Style and Speaker Attributes for TTS Style Transfer
- [ ] PITS: Variational Pitch Infer- ence without Fundamental Frequency for End-to-End Pitch-controllable TTS
- [ ] TriniTTS: Pitch-controllable End-to-end TTS without External Aligner
- [ ] ParaTTS: Learning Linguistic and Prosodic Cross-Sentence Information in Paragraph-Based TTS
- [ ] Phone-Level Prosody Modelling with GMM-Based MDN for Diverse and Controllable Speech Synthesis
- [ ] Language Model-Based Emotion Prediction Methods for Emotional Speech Synthesis Systems
- [ ] HiFiDenoise: High-Fidelity Denoising Text to Speech with Adversarial Networks
- [ ] HiddenSinger: High-Quality Singing Voice Synthesis via Neural Audio Codec and Latent Diffusion Models
- [x] EMOQ-TTS: Emotion Intensity Quantization for Fine-Grained Controllable Emotional Text- to-Speech
- [x] EmoSpeech: Guiding FastSpeech2 Towards Emotional Text to Speech
- [x] Emodiff: Intensity controllable emotional text-to-speech with soft-label guidance
- [ ] Adaspeech 4: Adaptive text to speech in zero-shot scenarios
- [ ] Emotional voice conversion: Theory, databases and esd
- [ ] Unsupervised word-level prosody tagging for controllable speech synthesis
- [ ] Msemotts: Multi- scale emotion transfer, prediction, and control for emotional speech synthesis
- [ ] Speech synthesis with mixed emotions
- [ ] BDDM: Bilateral denoising diffusion models for fast and high-quality speech synthesis
- [ ] Guided-tts: A diffusion model for text-to-speech via classifier guidance
- [ ] OverFlow: Putting flows on top of neural transducers for better TTS
- [x] EmoMix: Emotion Mixing via Diffusion Models for Emotional Speech Synthesis
- [x] EMOCONV-Diff: Diffusion-Based Speech Emotion Conversion for Non-Parallel and in-the-Wild Data
- [ ] Diffusion-Based Voice Conversion with Fast Maximum Likelihood Sampling Scheme
- [ ] Stylespeech: Self-Supervised Style Enhancing with VQ-VAE-Based Pre-Training for Expressive Audiobook Speech Synthesis
- [ ] In-the-wild Speech Emotion Conversion Using Disentangled Self-Supervised Representations and Neural Vocoder-based Resynthesis
- [ ] DelightfulTTS: The Microsoft Speech Synthesis System for Blizzard Challenge 2021
- [ ] DelightfulTTS 2: End-to-End Speech Synthesis with Adversarial Vector-Quantized Auto-Encoders
- [x] Self-supervised Context-aware Style Representation for Expressive Speech Synthesis
- [ ] Boosting fast and high-quality speech synthesis with linear diffusion
- [x] VoiceFlow- Efficient Text-to-Speech with Rectified Flow Matching
- [ ] Explicit Intensity Control for Accented Text-to-speech
- [x] Lightweight Zero-shot Text-to-Speech with Mixture of Adapters
- [x] SoundStorm- Efficient Parallel Audio Generation
- [x] High-Fidelity Audio Compression with Improved RVQGAN
- [x] VALL-E 2: Neural Codec Language Models are Human Parity Zero-Shot Text to Speech Synthesizers
- [x] Towards audio language modeling - an overview
- [x] P-Flow: A Fast and Data-Efficient Zero-Shot TTS through Speech Prompting
- [x] WenetSpeech4TTS- A 12,800-hour Mandarin TTS Corpus for Large Speech Generation Model Benchmark
- [x] Single-Codec- Single-Codebook Speech Codec towards High-PerformanceSpeech Generation
- [x] Fewer-Token Neural Speech Codec with Time-Invariant Codes
- [x] Laugh Now Cry Later: Controlling Time-Varying Emotional States of Flow-Matching-Based Zero-Shot Text-to-Speech
- [x] Voicebox: Text-Guided Multilingual Universal Speech Generation at Scale
- [x] Making flow-matching-based zero-shot text-to-speech laugh as you like
- [x] dMel- Speech Tokenization made Simple
- [ ] Latent Consistency Models: Synthesizing High-Resolution Images with Few-Step Inference
- [x] CLaM-TTS: Improving Neural Codec Language Model for Zero-Shot Text-to-Speech
- [x] Blockwise Parallel Decoding for Deep Autoregressive Models
- [x] NaturalSpeech 3: Zero-Shot Speech Synthesis with Factorized Codec and Diffusion Models
- [x] EmoSphere-TTS- Emotional Style and Intensity Modeling via Spherical Emotion Vector for Controllable Emotional Text-to-Speech
- [x] emotion2vec- Self-Supervised Pre-Training for Speech Emotion Representation
- [x] Large-scale Contrastive Language-Audio Pretraining with Feature Fusion and Keyword-to-Caption Augmentation
- [x] QA-MDT: Quality-aware Masked Diffusion Transformer for Enhanced Music Generation
- [x] Scalable Diffusion Models with Transformers
- [x] AudioLDM: Text-to-Audio Generation with Latent Diffusion Models
- [x] SSR-Speech: Towards Stable, Safe and Robust Zero-shot Text-based Speech Editing and Synthesis
- [x] E1 TTS: Simple and Fast Non-Autoregressive TTS
- [x] Autoregressive Diffusion Transformer forText-to-Speech Synthesis
- [x] An Investigation of Noise Robustness for Flow-Matching-Based Zero-Shot TTS
- [ ] 